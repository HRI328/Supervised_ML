{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMB7R4oz0NBrSan7Zy7q/G9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HRI328/Supervised_ML/blob/main/Supervised_Machine_Learning_Model_Builder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Supervised Machine Learning Model Builder**\n",
        "\n",
        "# With Flexible Feature Selection: Chi2 (categorical) + PCA (numerical)"
      ],
      "metadata": {
        "id": "BR1KgeFVFAkq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kpb1vVIb1-Pf",
        "outputId": "54b66de5-2e9d-4325-80db-e05b3b249c92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚠️  CatBoost not installed. Run: pip install catboost\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder, OrdinalEncoder\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.feature_selection import SelectKBest, f_classif, f_regression, chi2\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    confusion_matrix, mean_squared_error, mean_absolute_error, r2_score\n",
        ")\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "# Classification models\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# Regression models\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.svm import SVR\n",
        "\n",
        "# Boosting models\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "    XGBOOST_AVAILABLE = True\n",
        "except ImportError:\n",
        "    XGBOOST_AVAILABLE = False\n",
        "    print(\"⚠️  XGBoost not installed. Run: pip install xgboost\")\n",
        "\n",
        "try:\n",
        "    import catboost as cb\n",
        "    CATBOOST_AVAILABLE = True\n",
        "except ImportError:\n",
        "    CATBOOST_AVAILABLE = False\n",
        "    print(\"⚠️  CatBoost not installed. Run: pip install catboost\")\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Machine Learning Pipeline**"
      ],
      "metadata": {
        "id": "7lMcLcLEGWVn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class MLModelBuilder:\n",
        "    def __init__(self, task='classification', random_state=42):\n",
        "        self.task = task\n",
        "        self.random_state = random_state\n",
        "        self.models = {}\n",
        "        self.results = {}\n",
        "        self.best_model = None\n",
        "        self.scaler = StandardScaler()\n",
        "\n",
        "        # Preprocessing state\n",
        "        self.categorical_features = []\n",
        "        self.numerical_features = []\n",
        "        self.categorical_encoders = {}\n",
        "        self.feature_names = []\n",
        "\n",
        "        # Feature selection state\n",
        "        # Tracks what user chose: 'all', 'chi2', 'pca', or 'chi2+pca'\n",
        "        self.feature_mode = None\n",
        "        self.pca = None\n",
        "        self.chi2_selector = None\n",
        "        self.chi2_k = None\n",
        "\n",
        "        # Index tracking after encoding\n",
        "        self.num_indices = []\n",
        "        self.cat_indices = []\n",
        "\n",
        "        # Define models\n",
        "        if task == 'classification':\n",
        "            self.models = {\n",
        "                'Logistic Regression': LogisticRegression(random_state=random_state, max_iter=1000),\n",
        "                'Decision Tree': DecisionTreeClassifier(random_state=random_state),\n",
        "                'Random Forest': RandomForestClassifier(random_state=random_state, n_jobs=-1),\n",
        "                'Gradient Boosting': GradientBoostingClassifier(random_state=random_state),\n",
        "                'SVM': SVC(random_state=random_state, probability=True),\n",
        "                'KNN': KNeighborsClassifier()\n",
        "            }\n",
        "            if XGBOOST_AVAILABLE:\n",
        "                self.models['XGBoost'] = xgb.XGBClassifier(\n",
        "                    random_state=random_state, n_jobs=-1,\n",
        "                    eval_metric='logloss', use_label_encoder=False\n",
        "                )\n",
        "            if CATBOOST_AVAILABLE:\n",
        "                self.models['CatBoost'] = cb.CatBoostClassifier(\n",
        "                    random_state=random_state, verbose=0, thread_count=-1\n",
        "                )\n",
        "        else:\n",
        "            self.models = {\n",
        "                'Linear Regression': LinearRegression(),\n",
        "                'Ridge': Ridge(random_state=random_state),\n",
        "                'Lasso': Lasso(random_state=random_state),\n",
        "                'Decision Tree': DecisionTreeRegressor(random_state=random_state),\n",
        "                'Random Forest': RandomForestRegressor(random_state=random_state, n_jobs=-1),\n",
        "                'Gradient Boosting': GradientBoostingRegressor(random_state=random_state),\n",
        "                'SVR': SVR()\n",
        "            }\n",
        "            if XGBOOST_AVAILABLE:\n",
        "                self.models['XGBoost'] = xgb.XGBRegressor(random_state=random_state, n_jobs=-1)\n",
        "            if CATBOOST_AVAILABLE:\n",
        "                self.models['CatBoost'] = cb.CatBoostRegressor(\n",
        "                    random_state=random_state, verbose=0, thread_count=-1\n",
        "                )\n",
        "\n",
        "    # =========================================================================\n",
        "    # STEP 1: PREPARE DATA & ENCODE CATEGORICAL\n",
        "    # =========================================================================\n",
        "\n",
        "    def prepare_data(self, X, y, test_size=0.2):\n",
        "        \"\"\"\n",
        "        Split data and encode categorical features.\n",
        "        Does NOT scale yet — scaling happens after feature selection.\n",
        "        \"\"\"\n",
        "        print(\"=\" * 70)\n",
        "        print(\"DATA PREPARATION\")\n",
        "        print(\"=\" * 70)\n",
        "\n",
        "        if not isinstance(X, pd.DataFrame):\n",
        "            X = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(X.shape[1])])\n",
        "\n",
        "        self.feature_names = X.columns.tolist()\n",
        "        self.categorical_features = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "        self.numerical_features = X.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "        print(f\"✓ Numerical features ({len(self.numerical_features)}): {self.numerical_features}\")\n",
        "        print(f\"✓ Categorical features ({len(self.categorical_features)}): {self.categorical_features}\")\n",
        "\n",
        "        # Split\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X, y, test_size=test_size, random_state=self.random_state\n",
        "        )\n",
        "        print(f\"✓ Train: {X_train.shape[0]} | Test: {X_test.shape[0]}\")\n",
        "\n",
        "        # Encode categorical\n",
        "        if len(self.categorical_features) > 0:\n",
        "            X_train, X_test = self._encode(X_train, X_test)\n",
        "\n",
        "        # Convert to numpy\n",
        "        self.X_train = X_train.values.astype(float)\n",
        "        self.X_test = X_test.values.astype(float)\n",
        "        self.y_train = y_train\n",
        "        self.y_test = y_test\n",
        "\n",
        "        # Track indices after encoding\n",
        "        all_cols = list(X_train.columns)\n",
        "        self.num_indices = [i for i, c in enumerate(all_cols) if c in self.numerical_features]\n",
        "        self.cat_indices = [i for i, c in enumerate(all_cols) if c not in self.numerical_features]\n",
        "        self.encoded_col_names = all_cols\n",
        "\n",
        "        print(f\"✓ Numerical indices: {self.num_indices}\")\n",
        "        print(f\"✓ Categorical indices: {self.cat_indices}\")\n",
        "        print(f\"✓ Total features after encoding: {self.X_train.shape[1]}\")\n",
        "\n",
        "        return self.X_train, self.X_test, self.y_train, self.y_test\n",
        "\n",
        "    def _encode(self, X_train, X_test):\n",
        "        \"\"\"Encode categorical features using one-hot or label encoding.\"\"\"\n",
        "        print(\"\\n  Encoding categorical features...\")\n",
        "        X_train_enc = X_train.copy()\n",
        "        X_test_enc = X_test.copy()\n",
        "\n",
        "        for col in self.categorical_features:\n",
        "            n_unique = X_train[col].nunique()\n",
        "            if n_unique <= 10:\n",
        "                # One-hot encode\n",
        "                enc = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
        "                train_encoded = enc.fit_transform(X_train[[col]])\n",
        "                test_encoded = enc.transform(X_test[[col]])\n",
        "                new_cols = [f\"{col}_{cat}\" for cat in enc.categories_[0]]\n",
        "\n",
        "                train_df = pd.DataFrame(train_encoded, columns=new_cols, index=X_train_enc.index)\n",
        "                test_df = pd.DataFrame(test_encoded, columns=new_cols, index=X_test_enc.index)\n",
        "\n",
        "                X_train_enc = pd.concat([X_train_enc.drop(col, axis=1), train_df], axis=1)\n",
        "                X_test_enc = pd.concat([X_test_enc.drop(col, axis=1), test_df], axis=1)\n",
        "                self.categorical_encoders[col] = enc\n",
        "                print(f\"    • {col}: one-hot → {len(new_cols)} columns\")\n",
        "            else:\n",
        "                # Label encode\n",
        "                enc = LabelEncoder()\n",
        "                X_train_enc[col] = enc.fit_transform(X_train[col].astype(str))\n",
        "                X_test_enc[col] = enc.transform(X_test[col].astype(str))\n",
        "                self.categorical_encoders[col] = enc\n",
        "                print(f\"    • {col}: label encoded ({n_unique} unique)\")\n",
        "\n",
        "        return X_train_enc, X_test_enc\n",
        "\n",
        "    # =========================================================================\n",
        "    # STEP 2: FLEXIBLE FEATURE SELECTION\n",
        "    # User picks one of 4 modes:\n",
        "    #   Mode A: all numerical + all categorical (no selection)\n",
        "    #   Mode B: all numerical + chi2 selected categorical\n",
        "    #   Mode C: PCA numerical + all categorical\n",
        "    #   Mode D: PCA numerical + chi2 selected categorical\n",
        "    # =========================================================================\n",
        "\n",
        "    def select_features(self, mode='A', pca_variance=0.95, pca_n_components=None, chi2_k=5):\n",
        "        \"\"\"\n",
        "        Flexible feature selection. Pick a mode:\n",
        "\n",
        "        Args:\n",
        "            mode: One of:\n",
        "                'A' — All numerical + All categorical\n",
        "                'B' — All numerical + Chi2 selected categorical\n",
        "                'C' — PCA numerical + All categorical\n",
        "                'D' — PCA numerical + Chi2 selected categorical\n",
        "            pca_variance: Variance threshold for auto PCA (used if pca_n_components is None)\n",
        "            pca_n_components: Fixed number of PCA components (overrides pca_variance)\n",
        "            chi2_k: Number of categorical features to keep via Chi2\n",
        "        \"\"\"\n",
        "        mode = mode.upper()\n",
        "        assert mode in ['A', 'B', 'C', 'D'], \"Mode must be 'A', 'B', 'C', or 'D'\"\n",
        "        self.feature_mode = mode\n",
        "        self.chi2_k = chi2_k\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 70)\n",
        "        print(f\"FEATURE SELECTION — MODE {mode}\")\n",
        "        print(\"=\" * 70)\n",
        "        print(f\"  A = All num + All cat\")\n",
        "        print(f\"  B = All num + Chi2 cat\")\n",
        "        print(f\"  C = PCA num + All cat\")\n",
        "        print(f\"  D = PCA num + Chi2 cat\")\n",
        "        print(f\"  → Selected: Mode {mode}\")\n",
        "        print()\n",
        "\n",
        "        X_train_num = self.X_train[:, self.num_indices]\n",
        "        X_test_num = self.X_test[:, self.num_indices]\n",
        "        X_train_cat = self.X_train[:, self.cat_indices] if len(self.cat_indices) > 0 else None\n",
        "        X_test_cat = self.X_test[:, self.cat_indices] if len(self.cat_indices) > 0 else None\n",
        "\n",
        "        # --- Numerical part ---\n",
        "        if mode in ['C', 'D']:\n",
        "            X_train_num, X_test_num = self._apply_pca(X_train_num, X_test_num, pca_n_components, pca_variance)\n",
        "        else:\n",
        "            print(f\"✓ Numerical: keeping all {X_train_num.shape[1]} features\")\n",
        "\n",
        "        # --- Categorical part ---\n",
        "        if mode in ['B', 'D'] and X_train_cat is not None:\n",
        "            X_train_cat, X_test_cat = self._apply_chi2(X_train_cat, X_test_cat, chi2_k)\n",
        "        elif X_train_cat is not None:\n",
        "            print(f\"✓ Categorical: keeping all {X_train_cat.shape[1]} features\")\n",
        "        else:\n",
        "            print(\"✓ Categorical: none detected\")\n",
        "\n",
        "        # --- Combine ---\n",
        "        if X_train_cat is not None:\n",
        "            self.X_train = np.hstack([X_train_num, X_train_cat])\n",
        "            self.X_test = np.hstack([X_test_num, X_test_cat])\n",
        "        else:\n",
        "            self.X_train = X_train_num\n",
        "            self.X_test = X_test_num\n",
        "\n",
        "        # --- Scale after feature selection ---\n",
        "        self.scaler.fit(self.X_train)\n",
        "        self.X_train = self.scaler.transform(self.X_train)\n",
        "        self.X_test = self.scaler.transform(self.X_test)\n",
        "\n",
        "        print(f\"\\n✓ Scaling applied\")\n",
        "        print(f\"✓ Final shape — Train: {self.X_train.shape} | Test: {self.X_test.shape}\")\n",
        "\n",
        "        return self.X_train, self.X_test\n",
        "\n",
        "    def _apply_pca(self, X_train_num, X_test_num, n_components, variance_threshold):\n",
        "        \"\"\"Internal: apply PCA on numerical features only.\"\"\"\n",
        "        print(f\"✓ Numerical: applying PCA...\")\n",
        "\n",
        "        if n_components is None:\n",
        "            # Auto-select based on variance\n",
        "            pca_temp = PCA(random_state=self.random_state)\n",
        "            pca_temp.fit(X_train_num)\n",
        "            cumvar = np.cumsum(pca_temp.explained_variance_ratio_)\n",
        "            n_components = int(np.argmax(cumvar >= variance_threshold) + 1)\n",
        "            print(f\"    Auto-selected {n_components} components ({variance_threshold*100}% variance)\")\n",
        "        else:\n",
        "            n_components = min(n_components, X_train_num.shape[1])\n",
        "            print(f\"    Using {n_components} components (fixed)\")\n",
        "\n",
        "        self.pca = PCA(n_components=n_components, random_state=self.random_state)\n",
        "        X_train_pca = self.pca.fit_transform(X_train_num)\n",
        "        X_test_pca = self.pca.transform(X_test_num)\n",
        "\n",
        "        total_var = self.pca.explained_variance_ratio_.sum()\n",
        "        print(f\"    {X_train_num.shape[1]} features → {n_components} PCA components (variance: {total_var*100:.2f}%)\")\n",
        "\n",
        "        return X_train_pca, X_test_pca\n",
        "\n",
        "    def _apply_chi2(self, X_train_cat, X_test_cat, k):\n",
        "        \"\"\"Internal: apply Chi2 selection on categorical features only.\"\"\"\n",
        "        k = min(k, X_train_cat.shape[1])\n",
        "        print(f\"✓ Categorical: applying Chi2 (selecting {k} of {X_train_cat.shape[1]})...\")\n",
        "\n",
        "        # Chi2 requires non-negative values\n",
        "        cat_min = X_train_cat.min()\n",
        "        X_train_nn = X_train_cat - cat_min + 1e-10\n",
        "        X_test_nn = X_test_cat - cat_min + 1e-10\n",
        "\n",
        "        self.chi2_selector = SelectKBest(score_func=chi2, k=k)\n",
        "        X_train_sel = self.chi2_selector.fit_transform(X_train_nn, self.y_train)\n",
        "        X_test_sel = self.chi2_selector.transform(X_test_nn)\n",
        "\n",
        "        # Store offset for prediction\n",
        "        self.chi2_min_offset = cat_min\n",
        "\n",
        "        # Print selected features\n",
        "        selected_mask = self.chi2_selector.get_support()\n",
        "        cat_col_names = [self.encoded_col_names[i] for i in self.cat_indices]\n",
        "        selected_names = [name for name, sel in zip(cat_col_names, selected_mask) if sel]\n",
        "        scores = self.chi2_selector.scores_\n",
        "\n",
        "        print(f\"    Selected categorical features:\")\n",
        "        for name, sc in zip(selected_names, scores[selected_mask]):\n",
        "            print(f\"      • {name}: chi2 = {sc:.4f}\")\n",
        "\n",
        "        return X_train_sel, X_test_sel\n",
        "\n",
        "    def plot_pca_analysis(self, figsize=(15, 5)):\n",
        "        \"\"\"Visualize PCA results (only if PCA was used).\"\"\"\n",
        "        if self.pca is None:\n",
        "            print(\"✗ PCA not applied. Use mode 'C' or 'D' in select_features().\")\n",
        "            return\n",
        "\n",
        "        fig, axes = plt.subplots(1, 3, figsize=figsize)\n",
        "        ev = self.pca.explained_variance_ratio_\n",
        "\n",
        "        # 1. Per-component variance\n",
        "        axes[0].bar(range(1, len(ev) + 1), ev, color='steelblue', alpha=0.7)\n",
        "        axes[0].set_xlabel('Principal Component')\n",
        "        axes[0].set_ylabel('Variance Ratio')\n",
        "        axes[0].set_title('Variance by Component')\n",
        "        axes[0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "        # 2. Cumulative variance\n",
        "        cumvar = np.cumsum(ev)\n",
        "        axes[1].plot(range(1, len(cumvar) + 1), cumvar, marker='o', color='coral', linewidth=2)\n",
        "        axes[1].axhline(y=0.95, color='r', linestyle='--', label='95%')\n",
        "        axes[1].axhline(y=0.90, color='orange', linestyle='--', label='90%')\n",
        "        axes[1].set_xlabel('Components')\n",
        "        axes[1].set_ylabel('Cumulative Variance')\n",
        "        axes[1].set_title('Cumulative Variance')\n",
        "        axes[1].legend()\n",
        "        axes[1].grid(alpha=0.3)\n",
        "\n",
        "        # 3. 2D scatter (first 2 PCA components from training data)\n",
        "        # Re-extract numerical, apply PCA for plotting\n",
        "        X_num = self.X_train[:, :self.pca.n_components_] if self.feature_mode in ['C', 'D'] else None\n",
        "        if X_num is not None and X_num.shape[1] >= 2:\n",
        "            scatter = axes[2].scatter(X_num[:, 0], X_num[:, 1], c=self.y_train,\n",
        "                                      cmap='viridis', alpha=0.6, edgecolors='k', linewidth=0.5)\n",
        "            axes[2].set_xlabel(f'PC1 ({ev[0]*100:.1f}%)')\n",
        "            axes[2].set_ylabel(f'PC2 ({ev[1]*100:.1f}%)')\n",
        "            axes[2].set_title('PCA 2D Projection (Numerical)')\n",
        "            plt.colorbar(scatter, ax=axes[2], label='Target')\n",
        "        else:\n",
        "            axes[2].text(0.5, 0.5, 'Need ≥ 2 PCA components', ha='center', va='center')\n",
        "            axes[2].axis('off')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    # =========================================================================\n",
        "    # STEP 3: TRAIN, COMPARE, OPTIMIZE, SELECT\n",
        "    # =========================================================================\n",
        "\n",
        "    def train_all_models(self, cv=5):\n",
        "        \"\"\"Train all models with cross-validation.\"\"\"\n",
        "        print(\"\\n\" + \"=\" * 70)\n",
        "        print(\"TRAINING ALL MODELS\")\n",
        "        print(\"=\" * 70)\n",
        "\n",
        "        scoring = 'accuracy' if self.task == 'classification' else 'r2'\n",
        "\n",
        "        for name, model in self.models.items():\n",
        "            print(f\"\\n  ⚙️  {name}...\")\n",
        "            model.fit(self.X_train, self.y_train)\n",
        "\n",
        "            cv_scores = cross_val_score(model, self.X_train, self.y_train, cv=cv, scoring=scoring)\n",
        "            y_pred = model.predict(self.X_test)\n",
        "\n",
        "            if self.task == 'classification':\n",
        "                metrics = {\n",
        "                    'accuracy': accuracy_score(self.y_test, y_pred),\n",
        "                    'precision': precision_score(self.y_test, y_pred, average='weighted', zero_division=0),\n",
        "                    'recall': recall_score(self.y_test, y_pred, average='weighted', zero_division=0),\n",
        "                    'f1': f1_score(self.y_test, y_pred, average='weighted', zero_division=0),\n",
        "                    'cv_score': cv_scores.mean(),\n",
        "                    'cv_std': cv_scores.std()\n",
        "                }\n",
        "            else:\n",
        "                metrics = {\n",
        "                    'r2': r2_score(self.y_test, y_pred),\n",
        "                    'rmse': np.sqrt(mean_squared_error(self.y_test, y_pred)),\n",
        "                    'mae': mean_absolute_error(self.y_test, y_pred),\n",
        "                    'cv_score': cv_scores.mean(),\n",
        "                    'cv_std': cv_scores.std()\n",
        "                }\n",
        "\n",
        "            self.results[name] = {'model': model, 'metrics': metrics, 'predictions': y_pred}\n",
        "            print(f\"    CV: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})\")\n",
        "\n",
        "        print(\"\\n✓ All models trained\")\n",
        "\n",
        "    def compare_models(self):\n",
        "        \"\"\"Compare all trained models.\"\"\"\n",
        "        print(\"\\n\" + \"=\" * 70)\n",
        "        print(\"MODEL COMPARISON\")\n",
        "        print(\"=\" * 70)\n",
        "\n",
        "        rows = []\n",
        "        for name, res in self.results.items():\n",
        "            if 'metrics' in res:\n",
        "                row = {'Model': name}\n",
        "                row.update(res['metrics'])\n",
        "                rows.append(row)\n",
        "\n",
        "        df = pd.DataFrame(rows)\n",
        "        sort_col = 'accuracy' if self.task == 'classification' else 'r2'\n",
        "        df = df.sort_values(sort_col, ascending=False)\n",
        "        print(df.to_string(index=False))\n",
        "        return df\n",
        "\n",
        "    def optimize_model(self, model_name, param_grid, search_type='grid', cv=5, n_iter=50):\n",
        "        \"\"\"Optimize hyperparameters for a specific model.\"\"\"\n",
        "        print(\"\\n\" + \"=\" * 70)\n",
        "        print(f\"OPTIMIZING {model_name}\")\n",
        "        print(\"=\" * 70)\n",
        "\n",
        "        model = self.models[model_name]\n",
        "        scoring = 'accuracy' if self.task == 'classification' else 'r2'\n",
        "\n",
        "        if search_type == 'grid':\n",
        "            search = GridSearchCV(model, param_grid, cv=cv, scoring=scoring, n_jobs=-1, verbose=1)\n",
        "        else:\n",
        "            search = RandomizedSearchCV(model, param_grid, n_iter=n_iter, cv=cv,\n",
        "                                        scoring=scoring, n_jobs=-1, verbose=1, random_state=self.random_state)\n",
        "\n",
        "        search.fit(self.X_train, self.y_train)\n",
        "        y_pred = search.best_estimator_.predict(self.X_test)\n",
        "        test_score = accuracy_score(self.y_test, y_pred) if self.task == 'classification' else r2_score(self.y_test, y_pred)\n",
        "\n",
        "        print(f\"\\n✓ Best params: {search.best_params_}\")\n",
        "        print(f\"✓ Best CV score: {search.best_score_:.4f}\")\n",
        "        print(f\"✓ Test score: {test_score:.4f}\")\n",
        "\n",
        "        self.results[f\"{model_name} (Optimized)\"] = {\n",
        "            'model': search.best_estimator_,\n",
        "            'metrics': {'cv_score': search.best_score_, 'cv_std': 0},\n",
        "            'test_score': test_score\n",
        "        }\n",
        "        # Update accuracy/r2 in metrics for comparison\n",
        "        metric_key = 'accuracy' if self.task == 'classification' else 'r2'\n",
        "        self.results[f\"{model_name} (Optimized)\"]['metrics'][metric_key] = test_score\n",
        "\n",
        "        return search.best_estimator_, search.best_params_\n",
        "\n",
        "    def select_best_model(self):\n",
        "        \"\"\"Select best performing model.\"\"\"\n",
        "        print(\"\\n\" + \"=\" * 70)\n",
        "        print(\"SELECTING BEST MODEL\")\n",
        "        print(\"=\" * 70)\n",
        "\n",
        "        best_name, best_score = None, -np.inf\n",
        "        metric_key = 'accuracy' if self.task == 'classification' else 'r2'\n",
        "\n",
        "        for name, res in self.results.items():\n",
        "            score = res.get('metrics', {}).get(metric_key, res.get('test_score', -np.inf))\n",
        "            if score > best_score:\n",
        "                best_score = score\n",
        "                best_name = name\n",
        "\n",
        "        if best_name is None:\n",
        "            print(\"✗ No models found. Train models first.\")\n",
        "            return None, None\n",
        "\n",
        "        self.best_model = self.results[best_name]['model']\n",
        "        print(f\"✓ Best Model: {best_name}\")\n",
        "        print(f\"✓ Score: {best_score:.4f}\")\n",
        "        return self.best_model, best_name\n",
        "\n",
        "    # =========================================================================\n",
        "    # STEP 4: PREDICT — mirrors exact training pipeline\n",
        "    # =========================================================================\n",
        "\n",
        "    def _encode_new_data(self, X_new):\n",
        "        \"\"\"Encode categorical features in new data using saved encoders.\"\"\"\n",
        "        for col in self.categorical_features:\n",
        "            if col not in X_new.columns or col not in self.categorical_encoders:\n",
        "                continue\n",
        "            enc = self.categorical_encoders[col]\n",
        "            try:\n",
        "                if isinstance(enc, OneHotEncoder):\n",
        "                    encoded = enc.transform(X_new[[col]])\n",
        "                    new_cols = [f\"{col}_{cat}\" for cat in enc.categories_[0]]\n",
        "                    enc_df = pd.DataFrame(encoded, columns=new_cols, index=X_new.index)\n",
        "                    X_new = pd.concat([X_new.drop(col, axis=1), enc_df], axis=1)\n",
        "                elif isinstance(enc, LabelEncoder):\n",
        "                    known = set(enc.classes_)\n",
        "                    X_new[col] = X_new[col].astype(str).apply(lambda x: x if x in known else enc.classes_[0])\n",
        "                    X_new[col] = enc.transform(X_new[col])\n",
        "            except Exception as e:\n",
        "                print(f\"  ⚠️  Encoding error for {col}: {e}\")\n",
        "        return X_new\n",
        "\n",
        "    def predict(self, X_new, return_proba=False):\n",
        "        \"\"\"\n",
        "        Make predictions — applies the exact same pipeline as training:\n",
        "            Encode → Split (num | cat) → PCA/Chi2 → Combine → Scale → Predict\n",
        "\n",
        "        Args:\n",
        "            X_new: New data (DataFrame or numpy array)\n",
        "            return_proba: Return class probabilities (classification only)\n",
        "        \"\"\"\n",
        "        if self.best_model is None:\n",
        "            print(\"✗ No best model. Run select_best_model() first.\")\n",
        "            return None\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 70)\n",
        "        print(f\"PREDICTING — Mode {self.feature_mode}\")\n",
        "        print(\"=\" * 70)\n",
        "\n",
        "        # Convert to DataFrame\n",
        "        if isinstance(X_new, np.ndarray):\n",
        "            cols = self.feature_names if len(self.feature_names) == X_new.shape[1] else [f'feature_{i}' for i in range(X_new.shape[1])]\n",
        "            X_new = pd.DataFrame(X_new, columns=cols)\n",
        "        elif not isinstance(X_new, pd.DataFrame):\n",
        "            X_new = pd.DataFrame(X_new)\n",
        "\n",
        "        print(f\"✓ Input shape: {X_new.shape}\")\n",
        "\n",
        "        # Step 1: Encode categorical\n",
        "        if len(self.categorical_features) > 0:\n",
        "            print(\"✓ Step 1: Encoding categorical features...\")\n",
        "            X_new = self._encode_new_data(X_new)\n",
        "            print(f\"  Shape after encoding: {X_new.shape}\")\n",
        "\n",
        "        # Convert to numpy\n",
        "        X_array = X_new.values.astype(float)\n",
        "\n",
        "        # Step 2: Split into numerical and categorical using saved indices\n",
        "        X_num = X_array[:, self.num_indices]\n",
        "        X_cat = X_array[:, self.cat_indices] if len(self.cat_indices) > 0 else None\n",
        "        print(f\"✓ Step 2: Split — Num: {X_num.shape[1]} | Cat: {X_cat.shape[1] if X_cat is not None else 0}\")\n",
        "\n",
        "        # Step 3: Apply PCA on numerical (if mode C or D)\n",
        "        if self.feature_mode in ['C', 'D'] and self.pca is not None:\n",
        "            print(\"✓ Step 3: Applying PCA on numerical features...\")\n",
        "            X_num = self.pca.transform(X_num)\n",
        "            print(f\"  Numerical → {X_num.shape[1]} PCA components\")\n",
        "        else:\n",
        "            print(f\"✓ Step 3: Keeping all {X_num.shape[1]} numerical features\")\n",
        "\n",
        "        # Step 4: Apply Chi2 on categorical (if mode B or D)\n",
        "        if self.feature_mode in ['B', 'D'] and self.chi2_selector is not None and X_cat is not None:\n",
        "            print(\"✓ Step 4: Applying Chi2 on categorical features...\")\n",
        "            X_cat_nn = X_cat - self.chi2_min_offset + 1e-10\n",
        "            X_cat = self.chi2_selector.transform(X_cat_nn)\n",
        "            print(f\"  Categorical → {X_cat.shape[1]} selected features\")\n",
        "        elif X_cat is not None:\n",
        "            print(f\"✓ Step 4: Keeping all {X_cat.shape[1]} categorical features\")\n",
        "\n",
        "        # Step 5: Combine\n",
        "        if X_cat is not None:\n",
        "            X_combined = np.hstack([X_num, X_cat])\n",
        "        else:\n",
        "            X_combined = X_num\n",
        "        print(f\"✓ Step 5: Combined shape: {X_combined.shape}\")\n",
        "\n",
        "        # Step 6: Scale\n",
        "        print(\"✓ Step 6: Scaling...\")\n",
        "        X_scaled = self.scaler.transform(X_combined)\n",
        "\n",
        "        print(f\"✓ Final shape for prediction: {X_scaled.shape}\")\n",
        "\n",
        "        # Step 7: Predict\n",
        "        try:\n",
        "            predictions = self.best_model.predict(X_scaled)\n",
        "            print(f\"✓ Generated {len(predictions)} predictions\")\n",
        "        except Exception as e:\n",
        "            print(f\"✗ Prediction error: {e}\")\n",
        "            return None\n",
        "\n",
        "        # Probabilities\n",
        "        if return_proba and self.task == 'classification':\n",
        "            if hasattr(self.best_model, 'predict_proba'):\n",
        "                proba = self.best_model.predict_proba(X_scaled)\n",
        "                print(f\"✓ Probability shape: {proba.shape}\")\n",
        "                return predictions, proba\n",
        "            else:\n",
        "                print(\"⚠️  Model does not support predict_proba\")\n",
        "\n",
        "        return predictions\n",
        "\n",
        "    # =========================================================================\n",
        "    # STEP 5: EVALUATE & VISUALIZE\n",
        "    # =========================================================================\n",
        "\n",
        "    def evaluate_predictions(self, y_true, y_pred):\n",
        "        \"\"\"Evaluate predictions against true values.\"\"\"\n",
        "        print(\"\\n\" + \"=\" * 70)\n",
        "        print(\"PREDICTION EVALUATION\")\n",
        "        print(\"=\" * 70)\n",
        "\n",
        "        if self.task == 'classification':\n",
        "            metrics = {\n",
        "                'accuracy': accuracy_score(y_true, y_pred),\n",
        "                'precision': precision_score(y_true, y_pred, average='weighted', zero_division=0),\n",
        "                'recall': recall_score(y_true, y_pred, average='weighted', zero_division=0),\n",
        "                'f1': f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
        "            }\n",
        "            print(f\"  Accuracy:  {metrics['accuracy']:.4f}\")\n",
        "            print(f\"  Precision: {metrics['precision']:.4f}\")\n",
        "            print(f\"  Recall:    {metrics['recall']:.4f}\")\n",
        "            print(f\"  F1 Score:  {metrics['f1']:.4f}\")\n",
        "            print(f\"\\n  Confusion Matrix:\\n{confusion_matrix(y_true, y_pred)}\")\n",
        "        else:\n",
        "            metrics = {\n",
        "                'r2': r2_score(y_true, y_pred),\n",
        "                'rmse': np.sqrt(mean_squared_error(y_true, y_pred)),\n",
        "                'mae': mean_absolute_error(y_true, y_pred)\n",
        "            }\n",
        "            print(f\"  R² Score: {metrics['r2']:.4f}\")\n",
        "            print(f\"  RMSE:     {metrics['rmse']:.4f}\")\n",
        "            print(f\"  MAE:      {metrics['mae']:.4f}\")\n",
        "        return metrics\n",
        "\n",
        "    def plot_results(self, figsize=(15, 10)):\n",
        "        \"\"\"Plot model comparison and best model results.\"\"\"\n",
        "        if not self.results:\n",
        "            print(\"✗ No results. Train models first.\")\n",
        "            return\n",
        "\n",
        "        fig, axes = plt.subplots(2, 2, figsize=figsize)\n",
        "        metric_key = 'accuracy' if self.task == 'classification' else 'r2'\n",
        "        metric_label = 'Accuracy' if self.task == 'classification' else 'R² Score'\n",
        "\n",
        "        # Collect only models with metrics\n",
        "        names, scores, cv_scores, cv_stds = [], [], [], []\n",
        "        for name, res in self.results.items():\n",
        "            if 'metrics' in res and metric_key in res['metrics']:\n",
        "                names.append(name)\n",
        "                scores.append(res['metrics'][metric_key])\n",
        "                cv_scores.append(res['metrics'].get('cv_score', 0))\n",
        "                cv_stds.append(res['metrics'].get('cv_std', 0))\n",
        "\n",
        "        # 1. Model comparison\n",
        "        if names:\n",
        "            axes[0, 0].barh(names, scores, color='steelblue')\n",
        "            axes[0, 0].set_xlabel(metric_label)\n",
        "            axes[0, 0].set_title(f'Model Comparison — {metric_label}')\n",
        "            axes[0, 0].grid(axis='x', alpha=0.3)\n",
        "        else:\n",
        "            axes[0, 0].text(0.5, 0.5, 'No scores', ha='center')\n",
        "            axes[0, 0].axis('off')\n",
        "\n",
        "        # 2. CV scores\n",
        "        if names:\n",
        "            axes[0, 1].barh(names, cv_scores, xerr=cv_stds, color='coral', alpha=0.7)\n",
        "            axes[0, 1].set_xlabel('CV Score')\n",
        "            axes[0, 1].set_title('Cross-Validation Scores')\n",
        "            axes[0, 1].grid(axis='x', alpha=0.3)\n",
        "        else:\n",
        "            axes[0, 1].text(0.5, 0.5, 'No CV scores', ha='center')\n",
        "            axes[0, 1].axis('off')\n",
        "\n",
        "        # 3. Confusion matrix / Residual plot\n",
        "        best_model, best_name = self.select_best_model()\n",
        "        if best_model is not None:\n",
        "            try:\n",
        "                y_pred = best_model.predict(self.X_test)\n",
        "                if self.task == 'classification':\n",
        "                    cm = confusion_matrix(self.y_test, y_pred)\n",
        "                    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[1, 0])\n",
        "                    axes[1, 0].set_title(f'Confusion Matrix — {best_name}')\n",
        "                    axes[1, 0].set_ylabel('True')\n",
        "                    axes[1, 0].set_xlabel('Predicted')\n",
        "                else:\n",
        "                    residuals = self.y_test - y_pred\n",
        "                    axes[1, 0].scatter(y_pred, residuals, alpha=0.6)\n",
        "                    axes[1, 0].axhline(0, color='r', linestyle='--')\n",
        "                    axes[1, 0].set_xlabel('Predicted')\n",
        "                    axes[1, 0].set_ylabel('Residuals')\n",
        "                    axes[1, 0].set_title(f'Residual Plot — {best_name}')\n",
        "            except Exception as e:\n",
        "                axes[1, 0].text(0.5, 0.5, f'Plot error: {str(e)[:40]}', ha='center')\n",
        "                axes[1, 0].axis('off')\n",
        "        else:\n",
        "            axes[1, 0].text(0.5, 0.5, 'No best model', ha='center')\n",
        "            axes[1, 0].axis('off')\n",
        "\n",
        "        # 4. Feature importance / Actual vs Predicted\n",
        "        if best_model is not None:\n",
        "            try:\n",
        "                y_pred = best_model.predict(self.X_test)\n",
        "                if self.task == 'classification':\n",
        "                    if hasattr(best_model, 'feature_importances_'):\n",
        "                        imp = best_model.feature_importances_\n",
        "                        n = min(10, len(imp))\n",
        "                        idx = np.argsort(imp)[::-1][:n]\n",
        "                        axes[1, 1].barh(range(n), imp[idx], color='green', alpha=0.7)\n",
        "                        axes[1, 1].set_yticks(range(n))\n",
        "                        axes[1, 1].set_yticklabels([f'Feature {i}' for i in idx])\n",
        "                        axes[1, 1].set_xlabel('Importance')\n",
        "                        axes[1, 1].set_title(f'Feature Importance — {best_name}')\n",
        "                        axes[1, 1].invert_yaxis()\n",
        "                    elif hasattr(best_model, 'coef_'):\n",
        "                        coef = np.abs(best_model.coef_[0] if len(best_model.coef_.shape) > 1 else best_model.coef_)\n",
        "                        n = min(10, len(coef))\n",
        "                        idx = np.argsort(coef)[::-1][:n]\n",
        "                        axes[1, 1].barh(range(n), coef[idx], color='green', alpha=0.7)\n",
        "                        axes[1, 1].set_yticks(range(n))\n",
        "                        axes[1, 1].set_yticklabels([f'Feature {i}' for i in idx])\n",
        "                        axes[1, 1].set_xlabel('|Coefficient|')\n",
        "                        axes[1, 1].set_title(f'Coefficients — {best_name}')\n",
        "                        axes[1, 1].invert_yaxis()\n",
        "                    else:\n",
        "                        axes[1, 1].text(0.5, 0.5, 'Feature importance\\nnot available', ha='center')\n",
        "                        axes[1, 1].axis('off')\n",
        "                else:\n",
        "                    axes[1, 1].scatter(self.y_test, y_pred, alpha=0.6, edgecolors='k', linewidth=0.5)\n",
        "                    lo = min(self.y_test.min(), y_pred.min())\n",
        "                    hi = max(self.y_test.max(), y_pred.max())\n",
        "                    axes[1, 1].plot([lo, hi], [lo, hi], 'r--', lw=2, label='Perfect')\n",
        "                    axes[1, 1].set_xlabel('Actual')\n",
        "                    axes[1, 1].set_ylabel('Predicted')\n",
        "                    axes[1, 1].set_title(f'Actual vs Predicted — {best_name}')\n",
        "                    axes[1, 1].legend()\n",
        "            except Exception as e:\n",
        "                axes[1, 1].text(0.5, 0.5, f'Plot error: {str(e)[:40]}', ha='center')\n",
        "                axes[1, 1].axis('off')\n",
        "        else:\n",
        "            axes[1, 1].text(0.5, 0.5, 'No best model', ha='center')\n",
        "            axes[1, 1].axis('off')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    # =========================================================================\n",
        "    # SAVE / LOAD\n",
        "    # =========================================================================\n",
        "\n",
        "    def save_model(self, filepath='best_model.pkl'):\n",
        "        \"\"\"Save model and entire preprocessing pipeline.\"\"\"\n",
        "        import pickle\n",
        "        if self.best_model is None:\n",
        "            print(\"✗ No model to save.\")\n",
        "            return\n",
        "        package = {\n",
        "            'model': self.best_model,\n",
        "            'scaler': self.scaler,\n",
        "            'pca': self.pca,\n",
        "            'chi2_selector': self.chi2_selector,\n",
        "            'chi2_min_offset': getattr(self, 'chi2_min_offset', None),\n",
        "            'categorical_encoders': self.categorical_encoders,\n",
        "            'categorical_features': self.categorical_features,\n",
        "            'numerical_features': self.numerical_features,\n",
        "            'feature_names': self.feature_names,\n",
        "            'num_indices': self.num_indices,\n",
        "            'cat_indices': self.cat_indices,\n",
        "            'feature_mode': self.feature_mode,\n",
        "            'task': self.task\n",
        "        }\n",
        "\n",
        "        with open(filepath, 'wb') as f:\n",
        "            pickle.dump(package, f)\n",
        "        print(f\"✓ Model saved: {filepath}\")\n",
        "\n",
        "    def load_model(self, filepath='best_model.pkl'):\n",
        "        \"\"\"Load model and preprocessing pipeline.\"\"\"\n",
        "        import pickle\n",
        "        with open(filepath, 'rb') as f:\n",
        "            pkg = pickle.load(f)\n",
        "\n",
        "        self.best_model = pkg['model']\n",
        "        self.scaler = pkg['scaler']\n",
        "        self.pca = pkg['pca']\n",
        "        self.chi2_selector = pkg['chi2_selector']\n",
        "        self.chi2_min_offset = pkg.get('chi2_min_offset')\n",
        "        self.categorical_encoders = pkg['categorical_encoders']\n",
        "        self.categorical_features = pkg['categorical_features']\n",
        "        self.numerical_features = pkg['numerical_features']\n",
        "        self.feature_names = pkg['feature_names']\n",
        "        self.num_indices = pkg['num_indices']\n",
        "        self.cat_indices = pkg['cat_indices']\n",
        "        self.feature_mode = pkg['feature_mode']\n",
        "        self.task = pkg['task']\n",
        "\n",
        "        print(f\"✓ Model loaded: {filepath}\")\n",
        "        print(f\"✓ Feature mode: {self.feature_mode} | Task: {self.task}\")"
      ],
      "metadata": {
        "id": "0QtljCog2OCw"
      },
      "execution_count": 2,
      "outputs": []
    }
  ]
}